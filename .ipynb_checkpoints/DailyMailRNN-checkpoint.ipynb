{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "\n",
    "Given a sequence, predict the topic distribution for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# the dists are saved as comma separated values in a string (because pandas can't save datatype as int)\n",
    "# so to read as numeric-values, you must split on spaces and convert to float.\n",
    "# Michael says there is an efficient way to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>dists</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>By</td>\n",
       "      <td>0.000215766137942 0.000241668597233 0.00020097...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Beth Stebner Tara Brady</td>\n",
       "      <td>0.000117660461174 0.000123021350745 0.00012870...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PUBLISHED :</td>\n",
       "      <td>0.000574640958915 0.000137269205565 0.00015039...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13:57 EST , 21 January 2013</td>\n",
       "      <td>0.000117660461174 0.000123021350745 0.00012870...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>|</td>\n",
       "      <td>0.000215766137942 0.000241668597233 0.00020097...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     sentences  \\\n",
       "0                           By   \n",
       "1      Beth Stebner Tara Brady   \n",
       "2                  PUBLISHED :   \n",
       "3  13:57 EST , 21 January 2013   \n",
       "4                            |   \n",
       "\n",
       "                                               dists  \n",
       "0  0.000215766137942 0.000241668597233 0.00020097...  \n",
       "1  0.000117660461174 0.000123021350745 0.00012870...  \n",
       "2  0.000574640958915 0.000137269205565 0.00015039...  \n",
       "3  0.000117660461174 0.000123021350745 0.00012870...  \n",
       "4  0.000215766137942 0.000241668597233 0.00020097...  "
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods for cleaning up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalize_string(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And generating a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Voc:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2 # Count SOS and EOS\n",
    "      \n",
    "    def index_words(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.index_word(word)\n",
    "\n",
    "    def index_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TRAINING_CUTOFF = 900\n",
    "\n",
    "# # choose a random sentence and its corresponding distribution (label)\n",
    "# def choose_random_training_pair():\n",
    "#     training_pair = df.loc[random.randint(0, df.shape[0] - TRAINING_CUTOFF)]\n",
    "\n",
    "#     sentence = training_pair['sentences']\n",
    "#     distribution = training_pair['dists']\n",
    "\n",
    "#     return sentence, distribution\n",
    "\n",
    "# def choose_random_testing_pair():\n",
    "#     testing_pair = df.loc[random.randint(0, df.shape[0] + TRAINING_CUTOFF - 1)]\n",
    "\n",
    "#     sentence = testing_pair['sentences']\n",
    "#     distribution = testing_pair['dists']\n",
    "\n",
    "#     return sentence, distribution\n",
    "\n",
    "def generate_vocabulary(sentences):\n",
    "    # create the vocabulary\n",
    "    vocabulary = Voc('source_identification')\n",
    "    for sentence in sentences:\n",
    "        vocabulary.index_words(sentence)\n",
    "\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets define our Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DailyMailDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        \n",
    "        self._clean_df()\n",
    "        \n",
    "        self.max_sentence_size = int(self.df['sentences'].apply(lambda x: len(x.split(\" \"))).max())\n",
    "        \n",
    "        self.vocabulary = generate_vocabulary(self.sentences())\n",
    "        \n",
    "        if transform:\n",
    "            self.transform = ToTensor(self.vocabulary)\n",
    "        else:\n",
    "            self.transform = None\n",
    "    \n",
    "    def _clean_df(self):\n",
    "        # normalize sentences\n",
    "        self.df['sentences'] = self.df['sentences'].apply(normalize_string)\n",
    "        \n",
    "        # clean up any empty sentences\n",
    "        self.df = self.df[df['sentences'].str.strip() != '']\n",
    "        \n",
    "        # convert back to original distribution\n",
    "        self.df['dists'] = self.df['dists'].str.split(\" \")\n",
    "        self.df['dists'] = self.df['dists'].apply(lambda x: [float(i) for i in x])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.df.loc[idx].to_dict()\n",
    "        \n",
    "        if self.transform:\n",
    "            sample['max_sentence_size'] = self.max_sentence_size\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "    \n",
    "    def sentences(self):\n",
    "        return self.df['sentences'].tolist()\n",
    "    \n",
    "    def distributions(self):\n",
    "        return self.df['dists'].tolist()\n",
    "    \n",
    "    def head(self):\n",
    "        return self.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And some transformation functions to get our data in tensor/variable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a list of indexes, one for each word in the sentence\n",
    "def indexes_from_sentence(voc, sentence):\n",
    "    return [voc.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensor_from_sentence(voc, sentence, padding):\n",
    "    indexes = indexes_from_sentence(voc, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    sen_tensor = torch.LongTensor(indexes)\n",
    "    \n",
    "    # add padding\n",
    "    sen_tensor = torch.cat((sen_tensor, torch.LongTensor(padding).zero_()), 0)\n",
    "\n",
    "    return sen_tensor.view(-1, 1)\n",
    "\n",
    "def tensor_from_distribution(distribution):\n",
    "    return torch.FloatTensor(distribution).view(-1, 1) # TODO: convert to double tensor?\n",
    "\n",
    "class ToTensor(object):\n",
    "    def __init__(self, vocabulary):\n",
    "        self.vocabulary = vocabulary\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        sentence, distribution, max_sentence_size = sample['sentences'], sample['dists'], sample['max_sentence_size']\n",
    "        \n",
    "        padding = max_sentence_size - len(sentence.split(\" \"))\n",
    "        \n",
    "        return {'sentences': tensor_from_sentence(self.vocabulary, sentence, padding),\n",
    "                'dists': tensor_from_distribution(distribution)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>dists</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>by</td>\n",
       "      <td>[0.000215766137942, 0.000241668597233, 0.00020...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>beth stebner tara brady</td>\n",
       "      <td>[0.000117660461174, 0.000123021350745, 0.00012...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>published</td>\n",
       "      <td>[0.000574640958915, 0.000137269205565, 0.00015...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>est january</td>\n",
       "      <td>[0.000117660461174, 0.000123021350745, 0.00012...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>[0.000215766137942, 0.000241668597233, 0.00020...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 sentences                                              dists\n",
       "0                       by  [0.000215766137942, 0.000241668597233, 0.00020...\n",
       "1  beth stebner tara brady  [0.000117660461174, 0.000123021350745, 0.00012...\n",
       "2               published   [0.000574640958915, 0.000137269205565, 0.00015...\n",
       "3             est january   [0.000117660461174, 0.000123021350745, 0.00012...\n",
       "4                           [0.000215766137942, 0.000241668597233, 0.00020..."
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = DailyMailDataset('data.csv', transform=True)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dists': \n",
       " 1.00000e-04 *\n",
       "  2.1577\n",
       "  2.4167\n",
       "  2.0097\n",
       "    â‹®    \n",
       "  2.3395\n",
       "  1.8371\n",
       "  2.2585\n",
       " [torch.FloatTensor of size 4406x1], 'sentences': \n",
       "     2\n",
       "     1\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       " [torch.LongTensor of size 59x1]}"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4406"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUTPUT_SIZE = len(dataset.distributions()[0])\n",
    "OUTPUT_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3980"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_words = dataset.vocabulary.n_words\n",
    "n_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let the ML begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"6,7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "USE_CUDA = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # embedding contains input_size vectors of size hidden_size.\n",
    "        # this performs the word embeddings\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "        \n",
    "    def forward(self, word_inputs, hidden):\n",
    "        # Note: we run this all at once (over the whole input sequence)\n",
    "        seq_len, batch_size, _ = word_inputs.size()\n",
    "        \n",
    "        # embedding: (sentence length, batch size, word embedding length)\n",
    "        embedded = self.embedding(word_inputs).view(seq_len, batch_size, -1)\n",
    "        \n",
    "        # GRU:\n",
    "        #    input: (length of sequence, batch size, size of var in sequence)\n",
    "        #    output: \n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size))\n",
    "        if USE_CUDA: hidden = hidden.cuda()\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class classifierNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(classifierNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.i2o = nn.Linear(input_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "    \n",
    "    def forward(self, result_input):\n",
    "        output = self.i2o(result_input)\n",
    "        \n",
    "        batch_size = result_input.size()[1]\n",
    "        \n",
    "        output = self.softmax(output).view(-1, batch_size)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's our training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "clip = 5.0\n",
    "\n",
    "def train(input_var, target, rnn, rnn_optimizer, classifier_optimizer, criterion):\n",
    "\n",
    "    # Zero gradients of both optimizers\n",
    "    optimizer.zero_grad()\n",
    "    loss = 0 # Added onto for each word\n",
    "\n",
    "    # Get size of input and target sentences\n",
    "    input_length, batch_size, _ = input_var.size()\n",
    "\n",
    "    # Run sentence through rnn\n",
    "    hidden_var = rnn.init_hidden(batch_size)\n",
    "    output_var, hidden_var = rnn(input_var, hidden_var)\n",
    "    \n",
    "    # run classification rnn to get distribution\n",
    "    output_distribution = classifier(output_var[-1])\n",
    "    \n",
    "    # compute loss\n",
    "    loss = criterion(output_distribution, target)\n",
    "\n",
    "    # run backprop\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm(rnn.parameters(), clip)\n",
    "    \n",
    "    rnn_optimizer.step()\n",
    "    classifier_optimizer.step()\n",
    "    \n",
    "    return loss.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions for time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def as_minutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def time_since(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (as_minutes(s), as_minutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we actually train the RNN  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    "after we initialize stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 500 # size of each word embedding\n",
    "n_layers = 2 # number of layers for the RNN\n",
    "dropout_p = 0.05 # we never use this\n",
    "\n",
    "# Initialize the RNN\n",
    "rnn = RNN(n_words, hidden_size, n_layers)\n",
    "\n",
    "num_topics = 24\n",
    "\n",
    "# Initialize the classifier NN\n",
    "classifier = classifierNN(hidden_size, OUTPUT_SIZE)\n",
    "\n",
    "# Move models to GPU\n",
    "if USE_CUDA:\n",
    "    rnn.cuda()\n",
    "\n",
    "# Initialize optimizers and criterion\n",
    "learning_rate = 0.0001\n",
    "rnn_optimizer = optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "classifier_optimizer = optim.Adam(classifier.parameters(), lr=learning_rate)\n",
    "criterion = nn.KLDivLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Configuring training\n",
    "n_epochs = 1000\n",
    "plot_every = 20\n",
    "print_every = 10\n",
    "\n",
    "# Keep track of time elapsed and running averages\n",
    "plot_losses = []\n",
    "print_loss_total = 0 # Reset every print_every\n",
    "plot_loss_total = 0 # Reset every plot_every"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize data loader\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aaaaand now we actually start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-462-2da15fbfd8bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# Run the train function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_distribution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# Keep track of loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-457-fd65b5528984>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_var, target, rnn, rnn_optimizer, classifier_optimizer, criterion)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Run sentence through rnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mhidden_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0moutput_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# run classification rnn to get distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-455-9e24c19acbf3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, word_inputs, hidden)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# embedding: (sentence length, batch size, word embedding length)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# GRU:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         )\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/nn/_functions/thnn/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(cls, ctx, indices, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneeds_input_grad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Embedding doesn't \"\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;34m\"compute the gradient w.r.t. the indices\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    for i_batch, sample_batched in enumerate(dataloader):\n",
    "\n",
    "        # Get training data for this cycle\n",
    "        training_sentence, target_distribution = sample_batched['sentences'], sample_batched['dists']\n",
    "        \n",
    "        # convert to variables\n",
    "        training_sentence = Variable(training_sentence)\n",
    "        target_distribution = Variable(target_distribution)\n",
    "\n",
    "        # Run the train function\n",
    "        loss = train(training_sentence, target_distribution, rnn, rnn_optimizer, classifier_optimizer, criterion)\n",
    "\n",
    "        # Keep track of loss\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print_summary = '%s (%d %d%%) %.10f' % (time_since(start, epoch / n_epochs), epoch, epoch / n_epochs * 100, print_loss_avg)\n",
    "            print(print_summary)\n",
    "\n",
    "        if epoch % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110d9fc88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAAD8CAYAAABAWd66AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1165876a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "def show_plot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    loc = ticker.MultipleLocator(base=0.2) # put ticks at regular intervals\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "\n",
    "show_plot(plot_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets evaulate our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
