{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "\n",
    "Given a sequence, predict the topic distribution for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# the dists are saved as comma separated values in a string (because pandas can't save datatype as int)\n",
    "# so to read as numeric-values, you must split on spaces and convert to float.\n",
    "# Michael says there is an efficient way to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>dists</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>By</td>\n",
       "      <td>0.000215766137942 0.000241668597233 0.00020097...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Beth Stebner Tara Brady</td>\n",
       "      <td>0.000117660461174 0.000123021350745 0.00012870...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PUBLISHED :</td>\n",
       "      <td>0.000574640958915 0.000137269205565 0.00015039...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13:57 EST , 21 January 2013</td>\n",
       "      <td>0.000117660461174 0.000123021350745 0.00012870...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>|</td>\n",
       "      <td>0.000215766137942 0.000241668597233 0.00020097...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     sentences  \\\n",
       "0                           By   \n",
       "1      Beth Stebner Tara Brady   \n",
       "2                  PUBLISHED :   \n",
       "3  13:57 EST , 21 January 2013   \n",
       "4                            |   \n",
       "\n",
       "                                               dists  \n",
       "0  0.000215766137942 0.000241668597233 0.00020097...  \n",
       "1  0.000117660461174 0.000123021350745 0.00012870...  \n",
       "2  0.000574640958915 0.000137269205565 0.00015039...  \n",
       "3  0.000117660461174 0.000123021350745 0.00012870...  \n",
       "4  0.000215766137942 0.000241668597233 0.00020097...  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1174, 2)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods for cleaning up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalize_string(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And generating a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Voc:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2 # Count SOS and EOS\n",
    "      \n",
    "    def index_words(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.index_word(word)\n",
    "\n",
    "    def index_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING_CUTOFF = 900\n",
    "\n",
    "# # choose a random sentence and its corresponding distribution (label)\n",
    "# def choose_random_training_pair():\n",
    "#     training_pair = df.loc[random.randint(0, df.shape[0] - TRAINING_CUTOFF)]\n",
    "\n",
    "#     sentence = training_pair['sentences']\n",
    "#     distribution = training_pair['dists']\n",
    "\n",
    "#     return sentence, distribution\n",
    "\n",
    "# def choose_random_testing_pair():\n",
    "#     testing_pair = df.loc[random.randint(0, df.shape[0] + TRAINING_CUTOFF - 1)]\n",
    "\n",
    "#     sentence = testing_pair['sentences']\n",
    "#     distribution = testing_pair['dists']\n",
    "\n",
    "#     return sentence, distribution\n",
    "\n",
    "def generate_vocabulary(sentences):\n",
    "    # create the vocabulary\n",
    "    vocabulary = Voc('source_identification')\n",
    "    for sentence in sentences:\n",
    "        vocabulary.index_words(sentence)\n",
    "\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets define our Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DailyMailDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        \n",
    "        self._clean_df()\n",
    "        \n",
    "        self.max_sentence_size = int(self.df['sentences'].apply(lambda x: len(x.split(\" \"))).max())\n",
    "        \n",
    "        self.vocabulary = generate_vocabulary(self.sentences())\n",
    "        \n",
    "        if transform:\n",
    "            self.transform = ToTensor(self.vocabulary)\n",
    "        else:\n",
    "            self.transform = None\n",
    "    \n",
    "    def _clean_df(self):\n",
    "        # normalize sentences\n",
    "        self.df['sentences'] = self.df['sentences'].apply(normalize_string)\n",
    "        \n",
    "        # clean up any empty sentences\n",
    "        self.df = self.df[df['sentences'].str.strip() != '']\n",
    "        \n",
    "        # convert back to original distribution\n",
    "        self.df['dists'] = self.df['dists'].str.split(\" \")\n",
    "        self.df['dists'] = self.df['dists'].apply(lambda x: [float(i) for i in x])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.df.loc[idx].to_dict()\n",
    "        \n",
    "        if self.transform:\n",
    "            sample['max_sentence_size'] = self.max_sentence_size\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "    \n",
    "    def sentences(self):\n",
    "        return self.df['sentences'].tolist()\n",
    "    \n",
    "    def distributions(self):\n",
    "        return self.df['dists'].tolist()\n",
    "    \n",
    "    def head(self):\n",
    "        return self.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And some transformation functions to get our data in tensor/variable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a list of indexes, one for each word in the sentence\n",
    "def indexes_from_sentence(voc, sentence):\n",
    "    return [voc.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensor_from_sentence(voc, sentence, padding):\n",
    "    indexes = indexes_from_sentence(voc, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    sen_tensor = torch.LongTensor(indexes)\n",
    "    \n",
    "    # add padding\n",
    "    sen_tensor = torch.cat((sen_tensor, torch.LongTensor(padding).zero_()), 0)\n",
    "\n",
    "    return sen_tensor.view(-1, 1)\n",
    "\n",
    "def tensor_from_distribution(distribution):\n",
    "    return torch.FloatTensor(distribution).view(-1, 1) # TODO: convert to double tensor?\n",
    "\n",
    "class ToTensor(object):\n",
    "    def __init__(self, vocabulary):\n",
    "        self.vocabulary = vocabulary\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        sentence, distribution, max_sentence_size = sample['sentences'], sample['dists'], sample['max_sentence_size']\n",
    "        \n",
    "        padding = max_sentence_size - len(sentence.split(\" \"))\n",
    "        \n",
    "        return {'sentences': tensor_from_sentence(self.vocabulary, sentence, padding),\n",
    "                'dists': tensor_from_distribution(distribution)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>dists</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>by</td>\n",
       "      <td>[0.000215766137942, 0.000241668597233, 0.00020...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>beth stebner tara brady</td>\n",
       "      <td>[0.000117660461174, 0.000123021350745, 0.00012...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>published</td>\n",
       "      <td>[0.000574640958915, 0.000137269205565, 0.00015...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>est january</td>\n",
       "      <td>[0.000117660461174, 0.000123021350745, 0.00012...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>[0.000215766137942, 0.000241668597233, 0.00020...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 sentences                                              dists\n",
       "0                       by  [0.000215766137942, 0.000241668597233, 0.00020...\n",
       "1  beth stebner tara brady  [0.000117660461174, 0.000123021350745, 0.00012...\n",
       "2               published   [0.000574640958915, 0.000137269205565, 0.00015...\n",
       "3             est january   [0.000117660461174, 0.000123021350745, 0.00012...\n",
       "4                           [0.000215766137942, 0.000241668597233, 0.00020..."
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = DailyMailDataset('data.csv', transform=True)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dists': \n",
       " 1.00000e-04 *\n",
       "  2.1577\n",
       "  2.4167\n",
       "  2.0097\n",
       "    ⋮    \n",
       "  2.3395\n",
       "  1.8371\n",
       "  2.2585\n",
       " [torch.FloatTensor of size 4406x1], 'sentences': \n",
       "     2\n",
       "     1\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       "     0\n",
       " [torch.LongTensor of size 59x1]}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4406"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUTPUT_SIZE = len(dataset.distributions()[0])\n",
    "OUTPUT_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3980"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_words = dataset.vocabulary.n_words\n",
    "n_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let the ML begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"6,7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # embedding contains input_size vectors of size hidden_size.\n",
    "        # this performs the word embeddings\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "        \n",
    "    def forward(self, word_inputs, hidden):\n",
    "        # Note: we run this all at once (over the whole input sequence)\n",
    "        batch_size, seq_len = word_inputs.size()\n",
    "        \n",
    "        # embedding: (sentence length, batch size, word embedding length)\n",
    "        embedded = self.embedding(word_inputs)\n",
    "        embedded = embedded.transpose(0, 1)\n",
    "        \n",
    "        # GRU:\n",
    "        #    input: (length of sequence, batch size, size of var in sequence)\n",
    "        #    output: \n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size))\n",
    "        if USE_CUDA: hidden = hidden.cuda()\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class classifierNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(classifierNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.i2o = nn.Linear(input_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, result_input):\n",
    "        output = self.i2o(result_input)\n",
    "        output = self.softmax(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's our training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "clip = 5.0\n",
    "\n",
    "def train(input_var, target, rnn, rnn_optimizer, classifier_optimizer, criterion):\n",
    "\n",
    "    # Zero gradients of both optimizers\n",
    "    rnn_optimizer.zero_grad()\n",
    "    classifier_optimizer.zero_grad()\n",
    "    \n",
    "    loss = 0 # Added onto for each word\n",
    "\n",
    "    # Get size of input and target sentences\n",
    "    batch_size, input_length = input_var.size()\n",
    "\n",
    "    # Run sentence through rnn\n",
    "    hidden_var = rnn.init_hidden(batch_size)\n",
    "    output_var, hidden_var = rnn(input_var, hidden_var)\n",
    "    \n",
    "    # duke's trick: we hard code the last variable in the output to be\n",
    "    # a hidden representation of the distribution\n",
    "    hidden_distribution = output_var[-1]\n",
    "    \n",
    "    # run classification rnn to get distribution\n",
    "    output_distribution = classifier(output_var[-1])\n",
    "    \n",
    "    # compute loss\n",
    "    loss = criterion(output_distribution, target)\n",
    "\n",
    "    # run backprop\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm(rnn.parameters(), clip)\n",
    "    \n",
    "    rnn_optimizer.step()\n",
    "    classifier_optimizer.step()\n",
    "    \n",
    "    return loss.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions for time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def as_minutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def time_since(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (as_minutes(s), as_minutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we actually train the RNN  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    "after we initialize stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 500 # size of each word embedding\n",
    "batch_size = 4\n",
    "n_layers = 2 # number of layers for the RNN\n",
    "dropout_p = 0.05 # we never use this\n",
    "\n",
    "# Initialize the RNN\n",
    "rnn = RNN(n_words, hidden_size, n_layers)\n",
    "\n",
    "num_topics = 24\n",
    "\n",
    "# Initialize the classifier NN\n",
    "classifier = classifierNN(hidden_size, OUTPUT_SIZE)\n",
    "\n",
    "# Move models to GPU\n",
    "if USE_CUDA:\n",
    "    rnn.cuda()\n",
    "\n",
    "# Initialize optimizers and criterion\n",
    "learning_rate = 0.0001\n",
    "rnn_optimizer = optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "classifier_optimizer = optim.Adam(classifier.parameters(), lr=learning_rate)\n",
    "criterion = nn.KLDivLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring training\n",
    "n_epochs = 2\n",
    "plot_every = 20\n",
    "print_every = 10\n",
    "\n",
    "# Keep track of time elapsed and running averages\n",
    "plot_losses = []\n",
    "print_loss_total = 0 # Reset every print_every\n",
    "plot_loss_total = 0 # Reset every plot_every"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize data loader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aaaaand now we actually start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 1s (- 12m 29s) (1 0%) 0.0000142540\n",
      "0m 12s (- 10m 37s) (11 1%) 0.0001164592\n",
      "0m 21s (- 9m 46s) (21 3%) 0.0001065673\n",
      "0m 31s (- 9m 19s) (31 5%) 0.0001054681\n",
      "0m 40s (- 9m 0s) (41 6%) 0.0000936471\n",
      "0m 49s (- 8m 44s) (51 8%) 0.0001032879\n",
      "1m 0s (- 8m 41s) (61 10%) 0.0000977221\n",
      "1m 10s (- 8m 34s) (71 12%) 0.0000905169\n",
      "1m 20s (- 8m 24s) (81 13%) 0.0000846487\n",
      "1m 30s (- 8m 13s) (91 15%) 0.0000880641\n",
      "1m 41s (- 8m 11s) (101 17%) 0.0000878966\n",
      "1m 56s (- 8m 21s) (111 18%) 0.0000819623\n",
      "2m 11s (- 8m 25s) (121 20%) 0.0000816906\n",
      "2m 21s (- 8m 12s) (131 22%) 0.0000822565\n",
      "2m 32s (- 8m 4s) (141 23%) 0.0000760319\n",
      "2m 43s (- 7m 53s) (151 25%) 0.0000803067\n",
      "2m 54s (- 7m 43s) (161 27%) 0.0000804144\n",
      "3m 8s (- 7m 38s) (171 29%) 0.0000787862\n",
      "3m 18s (- 7m 27s) (181 30%) 0.0000799827\n",
      "3m 31s (- 7m 19s) (191 32%) 0.0000804650\n",
      "3m 41s (- 7m 7s) (201 34%) 0.0000801423\n",
      "3m 52s (- 6m 55s) (211 35%) 0.0000773734\n",
      "4m 4s (- 6m 45s) (221 37%) 0.0000808484\n",
      "4m 13s (- 6m 31s) (231 39%) 0.0000765847\n",
      "4m 22s (- 6m 17s) (241 40%) 0.0000755229\n",
      "4m 31s (- 6m 4s) (251 42%) 0.0000783358\n",
      "4m 40s (- 5m 51s) (261 44%) 0.0000802244\n",
      "4m 49s (- 5m 39s) (271 46%) 0.0000799641\n",
      "4m 58s (- 5m 26s) (281 47%) 0.0000786297\n",
      "5m 8s (- 5m 14s) (291 49%) 0.0000785804\n",
      "5m 12s (- 5m 10s) (295 50%) 0.0000300668\n",
      "5m 21s (- 4m 58s) (305 51%) 0.0000791101\n",
      "5m 30s (- 4m 46s) (315 53%) 0.0000816952\n",
      "5m 40s (- 4m 35s) (325 55%) 0.0000805445\n",
      "5m 52s (- 4m 26s) (335 56%) 0.0000791360\n",
      "6m 2s (- 4m 15s) (345 58%) 0.0000786887\n",
      "6m 13s (- 4m 5s) (355 60%) 0.0000764402\n",
      "6m 23s (- 3m 54s) (365 62%) 0.0000783589\n",
      "6m 42s (- 3m 48s) (375 63%) 0.0000769177\n",
      "6m 52s (- 3m 37s) (385 65%) 0.0000730762\n",
      "7m 2s (- 3m 26s) (395 67%) 0.0000795086\n",
      "7m 11s (- 3m 15s) (405 68%) 0.0000802846\n",
      "7m 23s (- 3m 4s) (415 70%) 0.0000778303\n",
      "7m 34s (- 2m 54s) (425 72%) 0.0000780405\n",
      "7m 47s (- 2m 44s) (435 73%) 0.0000820672\n",
      "7m 59s (- 2m 34s) (445 75%) 0.0000816015\n",
      "8m 11s (- 2m 23s) (455 77%) 0.0000782856\n",
      "8m 22s (- 2m 13s) (465 79%) 0.0000779990\n",
      "8m 34s (- 2m 2s) (475 80%) 0.0000802192\n",
      "8m 44s (- 1m 51s) (485 82%) 0.0000806596\n",
      "8m 55s (- 1m 40s) (495 84%) 0.0000769002\n",
      "9m 7s (- 1m 29s) (505 85%) 0.0000790235\n",
      "9m 20s (- 1m 19s) (515 87%) 0.0000770086\n",
      "9m 36s (- 1m 9s) (525 89%) 0.0000808105\n",
      "9m 49s (- 0m 58s) (535 90%) 0.0000789807\n",
      "10m 1s (- 0m 47s) (545 92%) 0.0000713121\n",
      "10m 14s (- 0m 36s) (555 94%) 0.0000753144\n",
      "10m 25s (- 0m 25s) (565 96%) 0.0000782800\n",
      "10m 36s (- 0m 14s) (575 97%) 0.0000791954\n",
      "10m 49s (- 0m 3s) (585 99%) 0.0000812395\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "total_cycles = len(dataloader) * n_epochs\n",
    "cycle = 0\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    for i_batch, sample_batched in enumerate(dataloader):\n",
    "        \n",
    "        cycle += 1\n",
    "\n",
    "        # Get training data for this cycle\n",
    "        training_sentence, target_distribution = sample_batched['sentences'], sample_batched['dists']\n",
    "        \n",
    "        # convert to variables\n",
    "        # we squeeze to get rid of the extra last dimension\n",
    "        training_sentence = Variable(training_sentence.squeeze(2))\n",
    "        target_distribution = Variable(target_distribution.squeeze(2))\n",
    "\n",
    "        # Run the train function\n",
    "        loss = train(training_sentence, target_distribution, rnn, rnn_optimizer, classifier_optimizer, criterion)\n",
    "\n",
    "        # Keep track of loss\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if i_batch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print_summary = '%s (%d %d%%) %.10f' % (time_since(start, cycle / total_cycles), cycle, cycle / total_cycles * 100, print_loss_avg)\n",
    "            print(print_summary)\n",
    "\n",
    "        if i_batch % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10adcb828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAAD8CAYAAABAWd66AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAHtdJREFUeJzt3Xl0m9WdN/Dv1WLJlmRbtuMttmxn\nD1ltnJBQoITSgbYByswUSMo25WXrRjt9een0dBvOMO2ZaTv0PfRtKEtTtgamFCgMbYF5CzQhe2xn\nT0gcr4nteJEtyZas5c4fkoKd2IkX6XmuxPdzjo8VRdZznzz21ze/uzxCSgkiItKfQe8GEBFRFAOZ\niEgRDGQiIkUwkImIFMFAJiJSBAOZiEgRDGQiIkUwkImIFMFAJiJShGkyLy4oKJCVlZVJagoRUXra\nvXt3t5RyxoVeN6lArqysxK5du6beKiKijyEhRPNEXseSBRGRIhjIRESKYCATESmCgUxEpAgGMhGR\nIhjIRESKYCATESlCqUD2BUJ4cWcLwhHeVoqIPn6UCuT/PtyFh17ehz80tOvdFCIizSkVyANDQQDA\n4+81gjdfJaKPG6UC2RsIAQAOd3jw7tHTOreGiEhbagWyPwQhgJIcKza8e1zv5hARaUqtQA6EYLeY\ncNdlVdh+ohd1LX16N4mISDNKBvK6lS7kZJrx+HuNejeJiEgzSgWyLxbINosJt62qwJ8PduD4aa/e\nzSIi0oRSgewNhGCzRLdovvMTlTAbDXjyr+wlE9HHg3KB7LBGA7nAbsEXLi7Dy7vb0TXg17llRETJ\np1Yg+6Mli7h7rpiFUCSCp7c06dcoIiKNqBXII0oWAFCRb8NnlpTg+W3N8PiDOraMiCj5lAvkkT1k\nALjvitnwBEJ4YXuLTq0iItKGMoEspTwzy2KkJWU5+MScfDy1+QQCobBOrSMiSj5lAnkoGEZEAnbr\nuTfCvu+Ts9HlCeC1upM6tIyISBvKBLLXH93H4uweMgBcNqcAi0qzseH944hwa04iSlPKBLInMH4g\nCyFw7ydno/G0D28f6tS6aUREmlAmkH3nCWQA+OziYpTnZWLDe8e5NScRpSVlAjlesrCNE8gmowF3\nXz4LdS1u7GzipkNElH7UCeRYD9kxxqBe3BcuLkeeLQMb3uPWnESUfpQL5PFKFgCQmWHEHasr8f8P\nd+FIh0erphERaUK5QB6vZBF3++oKZJqNePz98XvJgVAYhzsG8Fp9O/79z4fxg9f2wx/kHGYiUtv5\n009DEylZAIDTloFbVpbj2a3N+ObV8zAcjuBohwdHOj34sNOLI50enOj2nblztUEAEQksKMnGupWu\npJ8HEdFUqRPI/hCMBgGL6cKd9rsuq8IzW5tx+b/95cxzQgCuvCzMK3Lg2kXFmFtkx/xiB6oKbLjx\nFx9g45Ym3LKiHEKIZJ4GEdGUKRPI8WXTEwnMMmcWHvn8YjR2+zCvyIH5RQ7MKbQjM8M45uvv/EQl\n/s/v9mJrYw8unV2Q6KYTESWEMoHsGWMfi/O5ZRLlh+uXleLHfzyMjVuaGMhEpCx1BvX8kwvkybCa\njbhlRTneOdSJ1t7BpByDiGi6lAlk33BozI2FEuXWVRUQQuC5bc1JOwYR0XQoE8hef+iCU96mozQ3\nE9cuKsZvd7RgcDiUtOMQEU2VOoEcCMGRxEAGooN7A/4QXuU2nkSkIKUC2WYZe5ZEotRWOLGoNBsb\nPzjBDYqISDnqBLI/BLvFnNRjCCFwx6WVONrpxdbGnqQei4hospQI5EhEwjccTuqgXtz1y0qRZ8vA\nRt7JmogUo0Qg+4bjGwslt2QBRKfArVvJKXBEpB41AjkQ3fgn2SWLuPgUuGc5BY6IFKJEIHsDQQBI\n+qBeXElOJq5dXIxNnAJHRApRJJCjPeQL7fSWSHdeyilwRKQWNQL5zB2ntSlZAJwCR0TqUSOQNS5Z\nANEpcHfGp8Ad5xQ4ItKfIoEcK1lo2EMGgOtiU+B+/UGTpsclIhqLGoHs176HDESnwK1f6eIUOCJS\nghKB7BuOTXvTcFAv7ourXDBwChwRKUCJQPb4Q8gwGmAxadtDBjgFjojUoUQgewNBzcsVI/1DbArc\nK3XturWBiEiJQPYFtNnHYjwXVzixeGY2Nm5p4hQ4ItKNEvfU8/hDsGXo15ToFLgq/O//bMDz21tQ\n6LDAPRiEe2gY7sEg+gaD6B8aRp8vCPdQEP2Dw7CYjXhsfTUWlebo1m4iSi9KBLIvENJ0ld5Y1i4t\nwY//eAjffXX/qOdNBoHcLDNyszKQm2nGzFwrFpVmY/OH3bjz1zvx+/svRXlelk6tJqJ0okQgewMh\nFNgzdG2D1WzES/euRrt7CM6sDORkmpGbZYbdYoIQ4pzXf9jpwd9v2Irbn96B3923Gvl2iw6tJqJ0\nokQNOXq3EP1/N8yaYcflc2dg8cwclOdlwWE1jxnGADC3yIGn7qjFSfcQvrRxJ3wBztAgoulRJpD1\nLllMRW1lHh5bX4N97f34ygt7EAxH9G4SEaUwNQJZ50G96fj0RUV45MYlePfIaTz08l7O0iCiKdM9\nBcMRiaGgvtPepmvdShe6BgL4j3eOotBhxbc/s0DvJhFRCtI9Bb2B+NabujdlWr7+qTno8vix4b3j\nKHRY8KXLqvRuEhGlGN1TMF0CWQiBh29YjG5vAA+/cRAFDguuX1aqd7OIKIXoXkOOz05I5ZJFnNEg\n8PNbqrGyMg/feqkeW451690kIkohugeyJ3a3EBWmvSWC1WzEE3fUYlaBHfc+uxv72/v1bhIRpQjd\nAzneQ3akSSADQE6mGRu/tALZVhPu/PVOnOj26d0kIkoBugdyvIacLj3kuJKcTDxz10qEIhFc8x/v\n459+vw8tPdwEn4jGp0wgp/qg3ljmFDrwxtcuwxdqy/Dy7jas+em7+MeX6nGsy6t304hIQbqnYPyO\n06m4Um8iypxZeOTGJfjaVXPxq/cb8cKOZrxS147PLinBV9fMwcKS7Am/lz8Yxv72ftS3umE2GjC/\n2IEFxQ7kZk19HxB/MIxjXV4cP+2Fw2rC8nIn8mz67iuitZd2tqKx24eibAuKsq0oyrag0GFFYbZF\nl5sm0MeX7imYriWLsxXnWPH96y7Cl9fMxtObT+CZrc34r72ncPXCInz1qjlYXp476vVSSrT0DqKu\nxY26lj7Utbpx8OQAQpFzVwIWZ1vPhPP82MecQvuoMBkaDuP4aS+OdnrwYZcXH8Y+t/QO4uzFhRX5\nWVhennvm46LS7LQNpkAojO++uh/D4yx7d2aZUZRtRWG2FUUOCyrys3DD8pkpucNfMBxB42kfDncM\n4NApDw53DOBIhwf+YBgmowFmg4DJaIDJKGA2xD4bDTAbBUwGA2wWIyrybagqsGHWDBtmFdhRlG0Z\nd78XrXV7A/jj/g509vvxwNVzYTbqXgCYNDGZpb61tbVy165dCW3Aj948hI0fNOHIv3wmoe+ruv7B\nIDZ+0ISnt5xA/1AQl88twE215bEQ7kNdixs9vmEAQFaGEUvLclDjcqLa5cTy8lyEI/LMD9SRDg8O\ndXhwvMt7JliMBoGqAhtKcqxo7hlEa99HwWs2Rv9ubqEDc4vsmFsYDfBe3zDqW92ob+1DfasbnQMB\nAECG0YCFpdmoLs9FtSsXF1c4UeZMXCC19g7izX2nEIrIM0vPpQRk7DMASMgzj9cuLcHcIkdCjl3f\n6sbnf7EFv1hfg9Wz89E54EfngB9dA4HoY48fnQMBdA1EP3d6/ACANfMLcesqFz45rxBGgxqBNFKX\nx4/DsdA9fCr6/XGsy4NgOPqPaDYKzJ5hx4JiBxxWM0KRCIJhiVA4gmAk+jkUlqMeD/iDaOrxwR/8\n6JdXVoYxFtB2VBXYMHtGNLALHVY4rCZkZRiTGth9vmH86UAH3th7EluP9yDeX1l/iQuPfH6xMr8s\nhBC7pZS1F3yd3oH8nVf24c/7O7D7e59O6PumCm8ghOe2NePJvzai2xsN4FkzbLHwzUV1uRPziuww\nTeC3fTAcQVO3D4djIX24w4OOgSFU5kfDd16RHXOL7KjIt02o93Cqfwj1LW7Ut7pR1+rGvrZ+DAXD\nEAK4cflMfOua+ZiZmznlcx8cDuGX7x7H4+83Yjg08Y2Zrl1UjA23XTzl4460ccsJ/PD1g/jg21eh\ndALn0u4ewqYdLdi0sxWnPQHMzM3E+ktcuHlFOQoU2IJ1V1MvHn3nQ2weMQe+ONuKBSUOLCjOxsKS\n6P+gZhXYkWGafA8yEpHoGPDjRLcPjae9aOz2ofG0D43dXrT1DZ3zvy2jQcBuMSE70wSHxQyH1QSH\n1YzsTBOyrdEtbl15WajIz0JFvg35towLhmj/UBBvHejAG3tPYcuxboQiEpX5WVi7tBRrl5Xg1bqT\n2PDecXx/7UUJWTF70j2E1xtO4p4rZk054FMmkB/YVIf6Vjfee3BNQt831QwNh7H/ZD/mFtqnVRNO\nplA4gqOdXrzW0I5fb2kCANx1WRXuv3I2sq3mCb+PlBKv7z2FH715CKf6/bhheSkevGY+ZjgsEIh+\nwwsBxL/1hYg+KwTwjRfrsfV4D7Z/51MJ6f1888XoAp7Jvl8wHMFbBzrx3LZmbG3sgdkocO3iEty2\nqgIrKp2a98x2NvXi57EgLrBn4I7VlaitzMOCYgecGo0J+INhtPQOovG0D72+YXj8QQz4g/D4Q7GP\nIAZijweGgvD4g/AEQqNC3JZhhCvfhoq8LFQUZKEiz4aK/CzMzM1EXWsf3mg4hfc/PI1gWKLMmYnP\nLS3BdUtLsag0+8y/eSQicf/zu/H2wU48dccKrFlQOOVzau0dxPont8HtC+JP37xiyh2QlAnkuzbu\nRMeAH//19csT+r6UXG19g/jpW0fxSl078mwZ+PpVc/DFVRUX7Hnvb+/Hw68fxI6mXiyemY0fXrcI\ntZV5Ez7ubz5owg/+cACbH1qTkLLJmp+8izmFdjxx+wV/VsZ1rMuD57a14OU9bfD4Q5hXZMetqypw\nUUk2whEZ/ZASoYhEJP7n2HPhiITFZESNKxeF2dZJH3tnUy8efecothzrQYE9A/d9cja+eEkFMjNS\no+YfCIXR1jeElp5BNPf40NQziJbe6OPW3qFzavslOVZ8bkkJ1i4rxbKynHF/8Q0Oh/CFDVvR3DOI\n392/GguKJz54HtfSM4h1T2yDxx/Es3ddgmVnjfNMRsoE8k2PbwUAvHTv6oS+L2ljf3s//vXNQ/jg\neA+qCmx46Nr5uGZR8Tk/KD3eAH7y1lFs2tkCZ1YGHrxmPm6qLZ90/XVfWz+ue2wz/u+66mnvFeIe\nHMbyh9/Gg9fMx1fWzJnWewHREHi94SSe29aCfVNYoVlVYMPKyjysrIp+lDkzxw2cHSd68fP/Tt0g\nnohwrDzS3ONDW+8QZhfaUF3uhGGC3zMd/X5c/9hmmI0GvPqVT2CGY+IlpRPdPqz71Tb4Q2E8d9cl\nWDxzevfOnGgg6z61wRcIoXgKPQNSw+KZOXj+f12Cd4+cxr++eQj3PbcHtRVOfOdzC1HjciIYjuDZ\nrc149J2jGBwO4x8urcIDV89FTubESxwjLShxwGo2oK6lb9qBvLctGppnz3CZqqwME25e4cLNK1w4\neHIA3d4ATAYBg0HAGP8QIx7HPgaGgtjV1IftJ3rxpwMdeHFXKwCgNMeKlVV5uGRWPlZW5WFWgQ07\nm/pGBLEF3/3cwrQL4jijQWBmbma0TDB78l9fnGPFk3fU4qbHt+LeZ3fhhbtXwWq+8L/TsS4v1j+x\nDaGIxG/vXjWpqanTpXsgq3L7Jpo6IQTWLCjE5XML8J+72/Czt4/ib//fB7hmUREaT/vwYZcXV8yb\nge+vXYg5hdObHWE2GrC0LBd7WtzTbnd9qxtCAEvKEn/n8ItKJ/dDXO1y4u4rZiESkTja5cGOE73Y\nfqIXm4/14NX6kwCAbKsJA/4QCuwWfG/tRVi/0pWWQZxIS8ty8bObluPLz+/BQy/vxaM3Lz9vff9I\nhwdffHIbAIFN96zCvATN5pko3ZPQFwilxU5vBJiMBqxb6cL1y0rxxF8b8av3GzHDYcGTt9fiUwsL\nEzbQVe3KxdObT8AfDE+oxzOehlY3Zs+wT2pAMtkMBoEFxdlYUJyN21dXQkqJpp5B7DjRgz3Nbswv\ndmAdg3hSPrukBA9eMx///ucjmD3Djq9/au6Yrzt4cgC3PrUdJoPAC3evwpxCu8YtVSCQPf5QWm0s\nRNFFPt+4eh7uv3I2TAZDwufp1riceDzciP3t/ZMaEBxJSon6VjeunD/1EXgtCBGdM15VYMPNK1x6\nNydlffnK2Tje5cXP3j6KWTNsWLt0dLlrf3s/bn1qOzLNRrxw9ypUFdh0aaeuS1mC4QgCoQhLFmnK\nYjImZdFEjcsJANjT0jfl92jrG0KPbxjLXYmpH5PahBD40d8tQW2FE996qQH1rR+VvBpa3Vj/xDbY\nMkx48Z7VuoUxoHMg+9J4YyFKnhkOC8rzMrGneep15Ia26NcuL2Mgf1xYTEY8ftvFmOGw4O5nduGk\newi7m/tw65PbkZNlxov3roIrX98l8boGcnxzegYyTVaNy4k9LX1Tvst3fYsbGaboBk308ZFvt+Dp\nO1dgaDiM257ajtuf2o58ewZeund1QrcDmCp9e8jD6XP7JtJWjcuJLk8AJ/v9U/r6hjY3FpdmT2n5\nMKW2eUUOPLa+Gie6fSjKseLFe1ejJGfqWwAkkq5J6GUPmaaoOlb73dPcN+nlrMFwBPva+7FuJQfJ\nPq6unF+IN752OWY6M6c8Jz4Z9C1ZfEy23qTEW1iSDavZMKWBvaOdHviDkYQtCKHUdFFptlJhDOhd\nsgik9+b0lDxmowFLZ05tgUhDa2JX6BEliq6B7E2zO06TtqorcnHwZD/8wfCkvq6+tQ/O2LaPRCrR\nN5A57Y2mIbpXhsSBk5PbyKehtR/LynOV2bycKE6JQLZxGShNwUcDexMvW3gDIRzt8mAZ5x+TgnQv\nWWSajRO6GwbR2QodVpQ5Myc1sLevrR9Sgiv0SEm6z0PmHGSajskuEImv0GMPmVSk+0o91o9pOmpc\nuegcCODUBBeI1Le4UZGfhTyNbmtENBm6T3tjINN01FRMbqOhhjY3e8ekLN0H9WwWDujR1J1ZIDKB\ngb3OAT9O9fundW80omTSOZDDsFvUWilDqeWjBSIX7iHHt1zkghBSlc6BHOQqPZq2alcuDkxggUhD\nqxsmg8CiSd5eiUgruk97Y8mCpqv6zAKRgfO+rr7VHStx8HuO1KTzoB5LFjR9NRXREkTdecoWkYjE\n3rZ+LCtP/A1NiRJFt0AOhMIYDkdgZw+ZpmkiC0SOn/bCGwhxhgUpTbdA9gWi9T5Oe6NEqHE5zzvT\nIj6gV80VeqQw3QL5zOb0Ct2CnVJXjSsXHQN+nHQPjfn3DW1uOCwmzCrQ/tbuRBOlWyB7AkEAYMmC\nEqL6Aneirm91Y2l5DgxJuAs2UaIoULJgD5mmb2FJNiwmA+rG2LDeHwzj8Cnu8Ebq069kEeshc9ob\nJUKGyYClZTlj9pAPnOxHKCK5Qo+Up2MgR3vIXBhCiVLjcuJA+wACodELROpjt2yqZiCT4vQf1GPJ\nghKk2uXEcDiC/e2jF4g0tLpRkmNFYbZVp5YRTQxLFpQ2alxjLxCpb3Vz/wpKCbqXLGwZLFlQYhRm\nWzEzN3PUwF6vbxgtvYOsH1NK0LVkYcswchoSJVRNhXPUwF5DK+8QQqlDx2lvvH0TJV6NKxen+v04\n1R9dIFLf6oZBAEvLuIcFqU/HkkUINi6bpgSriS8QiS2jbmhzY26hg99rlBJ0XKkXgoM/JJRg8QUi\n8RufNnBAj1IISxaUVuILROpa+tDSO4i+wSAH9Chl6Dyox0CmxKt2ObG/fQA7TvQCAPdAppShaw2Z\nPWRKhhpXLobDETy/vQVWswHzixx6N4loQvQNZNaQKQniA3v1rW4smZkDk1HXG+MQTZgu36lSSgYy\nJU18gQjAO0xTatElkAOhCMIRyZIFJU1NRbSXzAE9SiW6BLLnzMZCDGRKjkuq8mAQH21cT5QKdElE\nX4CBTMl1y4pyrKzKO1O6IEoFuvSQvbFA5uopShaT0YB5nF1BKUbXQOZKPSKij+gTyGfuOM1AJiKK\nY8mCiEgRLFkQESmCPWQiIkXoEsi+QAhCAFkZvJ8eEVGcbgtD7BYThODtm4iI4nQrWXBRCBHRaLqV\nLBjIRESj6dZD5oAeEdFougWyg4tCiIhG0W2lHksWRESjsWRBRKQIzrIgIlKE5oEspeQsCyKiMWge\nyEPBMCKSO70REZ1N80COb73JGjIR0WiaB7KHO70REY1J80Dm/fSIiMbGkgURkSK0D+R4yYKDekRE\no+gWyOwhExGNplsgs4ZMRDQaSxZERIrQZVDPaBCwmHRZtU1EpCxdpr3x9k1EROfSZWEI68dEROfS\nrYdMRESj6TKox42FiIjOpcugHucgExGdS5ceMjcWIiI6ly6BbLMYtT4sEZHydBjUC8NuMWt9WCIi\n5WkayJGI5KAeEdE4NA1k33B8HwuWLIiIzqZtIAfCAMCSBRHRGDQNZG8gCAAc1CMiGoPGgRztIXOn\nNyKic2kbyP54DZklCyKis7FkQUSkCH1KFuwhExGdQ+OSBXvIRETj0XgecmzaGwf1iIjOoWkge/wh\nmI0CFhN7yEREZ9N8UI+b0xMRjU3zlXosVxARjU3zkoUtg4FMRDQWjXvIIa7SIyIah8Y1ZN6+iYho\nPJr3kDmoR0Q0Nm1ryCxZEBGNS/PNhTioR0Q0Ns0CORyRGApy2hsR0Xg0C2RvIL71JgOZiGgsmgWy\nj4FMRHRe2veQWbIgIhqTZoHsid0thPOQiYjGpnnJwsFAJiIak+YlC/aQiYjGxlkWRESK0C6QYzVk\nrtQjIhobSxZERIrQdFDPYjLAbNR0tTYRUcrQbtobd3ojIjovTXvIXBRCRDQ+TQf12EMmIhqfpiUL\nDugREY1P05IFV+kREY1P02lv7CETEY2Pg3pERIrQdLc3DuoREY1Pk0AOhiMIhCIMZCKi89AkkHm3\nECKiC9MkkOOb0zOQiYjGp00PeZi3byIiuhBNAtnL2zcREV2QNoHMGjIR0QVpGsjcnJ6IaHwsWRAR\nKYIlCyIiRWgayLYMoxaHIyJKSZotDMk0G2Hi7ZuIiMalWQ+Zc5CJiM5Ps5V6rB8TEZ2fZiULBjIR\n0flpVrKwWTigR0R0PhoFchh2i1mLQxERpSxN6giXzs5HSY5Vi0MREaUsTQL5e2sv0uIwREQpjROD\niYgUwUAmIlIEA5mISBEMZCIiRTCQiYgUwUAmIlIEA5mISBEMZCIiRQgp5cRfLMRpAM1TPFYBgO4p\nfq2K0u18gPQ7p3Q7HyD9zindzgcY+5wqpJQzLvSFkwrk6RBC7JJS1mpyMA2k2/kA6XdO6XY+QPqd\nU7qdDzC9c2LJgohIEQxkIiJFaBnIv9LwWFpIt/MB0u+c0u18gPQ7p3Q7H2Aa56RZDZmIiM6PJQsi\nIkUkPZCFENcKIY4IIY4JIb6d7ONpQQjRJITYJ4SoF0Ls0rs9UyGEeFoI0SWE2D/iuTwhxNtCiA9j\nn516tnEyxjmfHwoh2mPXqV4I8Vk92zgZQohyIcRfhBAHhRAHhBAPxJ5P5Ws03jml5HUSQliFEDuE\nEA2x8/nn2PNVQojtscx7UQiRMeH3TGbJQghhBHAUwKcBtAHYCWCdlPJg0g6qASFEE4BaKWXKzp8U\nQlwBwAvgGSnl4thz/wagV0r549gvT6eU8iE92zlR45zPDwF4pZQ/0bNtUyGEKAFQIqXcI4RwANgN\n4PMA7kTqXqPxzukmpOB1EkIIADYppVcIYQawGcADAP4RwO+llJuEEBsANEgpfzmR90x2D3klgGNS\nykYp5TCATQBuSPIxaQKklO8D6D3r6RsA/Cb2+DeI/rCkhHHOJ2VJKU9JKffEHnsAHAIwE6l9jcY7\np5Qko7yxP5pjHxLAVQB+F3t+Utco2YE8E0DriD+3IYUvwAgSwFtCiN1CiHv0bkwCFUkpT8UedwAo\n0rMxCfJVIcTeWEkjZf57P5IQohJANYDtSJNrdNY5ASl6nYQQRiFEPYAuAG8DOA7ALaUMxV4yqczj\noN7UXCalrAHwGQBfif13Oa3IaC0r1afg/BLAbADLAZwC8FN9mzN5Qgg7gJcBfENKOTDy71L1Go1x\nTil7naSUYSnlcgBliFYEFkzn/ZIdyO0Aykf8uSz2XEqTUrbHPncBeAXRC5EOOmN1vni9r0vn9kyL\nlLIz9gMTAfAEUuw6xeqSLwN4Xkr5+9jTKX2NxjqnVL9OACCldAP4C4DVAHKFEPEbSE8q85IdyDsB\nzI2NOmYAuAXAH5J8zKQSQthiAxIQQtgA/A2A/ef/qpTxBwB3xB7fAeA1HdsybfHgirkRKXSdYgNG\nTwE4JKX82Yi/StlrNN45pep1EkLMEELkxh5nIjp54RCiwfz3sZdN6holfWFIbArLowCMAJ6WUj6S\n1AMmmRBiFqK9YgAwAXghFc9JCPFbAFciujNVJ4AfAHgVwEsAXIju6neTlDIlBsrGOZ8rEf1vsATQ\nBODeEfVXpQkhLgPwVwD7AERiT38H0Zprql6j8c5pHVLwOgkhliI6aGdEtHP7kpTy4VhGbAKQB6AO\nwK1SysCE3pMr9YiI1MBBPSIiRTCQiYgUwUAmIlIEA5mISBEMZCIiRTCQiYgUwUAmIlIEA5mISBH/\nA6Y/YEmCsF+8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x127827c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "def show_plot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    loc = ticker.MultipleLocator(base=0.2) # put ticks at regular intervals\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "\n",
    "show_plot(plot_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets evaulate our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, target_distribution):\n",
    "    # Get size of input and target sentences\n",
    "    batch_size, input_length = sentence.size()\n",
    "\n",
    "    # Run sentence through rnn\n",
    "    hidden_var = rnn.init_hidden(batch_size)\n",
    "    output_var, hidden_var = rnn(sentence, hidden_var)\n",
    "    \n",
    "    # duke's trick: we hard code the last variable in the output to be\n",
    "    # a hidden representation of the distribution\n",
    "    hidden_distribution = output_var[-1]\n",
    "    \n",
    "    # run classification rnn to get distribution\n",
    "    output_distribution = classifier(output_var[-1])\n",
    "    \n",
    "    # compute loss (not sure what other metric to evaluate on)\n",
    "    loss = criterion(output_distribution, target_distribution)\n",
    "    \n",
    "    return loss.data[0], output_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2300482669379562e-05\n",
      "Variable containing:\n",
      "-7.6346 -8.7385 -8.5356  ...  -8.6939 -8.3758 -8.6114\n",
      "[torch.FloatTensor of size 1x4406]\n",
      "\n",
      "Variable containing:\n",
      "1.00000e-04 *\n",
      " 2.1577  2.4167  2.0097  ...   2.3395  1.8371  2.2585\n",
      "[torch.FloatTensor of size 1x4406]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence, distribution = dataset[0]['sentences'], dataset[0]['dists']\n",
    "\n",
    "sentence_var = Variable(sentence.transpose(0, 1))\n",
    "distribution_var = Variable(distribution.transpose(0, 1))\n",
    "loss, output_var = evaluate(sentence_var, distribution_var)\n",
    "\n",
    "print(loss)\n",
    "print(output_var)\n",
    "print(distribution_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
