{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "\n",
    "Given a sequence, predict the topic distribution for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# the dists are saved as comma separated values in a string (because pandas can't save datatype as int)\n",
    "# so to read as numeric-values, you must split on spaces and convert to float.\n",
    "# Michael says there is an efficient way to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>dists</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>At least 100 families receiving housing benefi...</td>\n",
       "      <td>-6.63052740918379 -7.265516111874736 -6.913763...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>More 30 families given staggering # 1,500 week...</td>\n",
       "      <td>-7.830457309882772 -7.835212219711822 -5.67915...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Of 100 families , 60 rent paid state value # 5...</td>\n",
       "      <td>-7.872029012645258 -6.913763006152177 -5.67915...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>At time millions people struggling get housing...</td>\n",
       "      <td>-6.63052740918379 -6.443461174803561 -7.839491...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Luxury : The kind upmarket homes Kensington , ...</td>\n",
       "      <td>-6.899740172840747 -inf -4.813336632402851 -7....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentences  \\\n",
       "0  At least 100 families receiving housing benefi...   \n",
       "1  More 30 families given staggering # 1,500 week...   \n",
       "2  Of 100 families , 60 rent paid state value # 5...   \n",
       "3  At time millions people struggling get housing...   \n",
       "4  Luxury : The kind upmarket homes Kensington , ...   \n",
       "\n",
       "                                               dists  \n",
       "0  -6.63052740918379 -7.265516111874736 -6.913763...  \n",
       "1  -7.830457309882772 -7.835212219711822 -5.67915...  \n",
       "2  -7.872029012645258 -6.913763006152177 -5.67915...  \n",
       "3  -6.63052740918379 -6.443461174803561 -7.839491...  \n",
       "4  -6.899740172840747 -inf -4.813336632402851 -7....  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert back to original distribution\n",
    "df['dists'] = df['dists'].str.split(\" \")\n",
    "df['dists'] = df['dists'].apply(lambda x: [np.e ** float(i) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02244788873638659"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(df['dists'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let the ML begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"6,7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "USE_CUDA = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our vocabulary class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Voc:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2 # Count SOS and EOS\n",
    "      \n",
    "    def index_words(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.index_word(word)\n",
    "\n",
    "    def index_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods for cleaning up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalize_string(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets actually use the data now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SENTENCES = 200\n",
    "\n",
    "# subset of the data for training\n",
    "training_df = df[:MAX_SENTENCES]\n",
    "\n",
    "# choose a random sentence and its corresponding distribution (label)\n",
    "def choose_random_training_pair():\n",
    "    training_pair = training_df.loc[random.randint(0, training_df.shape[0])]\n",
    "\n",
    "    sentence = training_pair['sentences']\n",
    "    distribution = training_pair['dists']\n",
    "\n",
    "    return sentence, distribution\n",
    "\n",
    "def generate_vocabulary(sentences):\n",
    "    # create the vocabulary\n",
    "    vocabulary = Voc('source_identification')\n",
    "    for sentence in sentences:\n",
    "        vocabulary.index_words(sentence)\n",
    "\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary = generate_vocabulary(training_df['sentences'].tolist())\n",
    "n_words = vocabulary.n_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets actually do some ML now.\n",
    "\n",
    "Here are some sentences --> variable functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Return a list of indexes, one for each word in the sentence\n",
    "def indexes_from_sentence(voc, sentence):\n",
    "    return [voc.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def variable_from_sentence(voc, sentence):\n",
    "    indexes = indexes_from_sentence(voc, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    var = Variable(torch.LongTensor(indexes).view(-1, 1))\n",
    "    if USE_CUDA: var = var.cuda()\n",
    "    return var\n",
    "\n",
    "def tensor_from_distribution(distribution):\n",
    "    return Variable(torch.FloatTensor(distribution).view(-1, 1), requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now here's the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # embedding contains input_size vectors of size hidden_size.\n",
    "        # this performs the word embeddings\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "        \n",
    "    def forward(self, word_inputs, hidden):\n",
    "        # Note: we run this all at once (over the whole input sequence)\n",
    "        seq_len = len(word_inputs)\n",
    "        \n",
    "        # embedding: (sentence length, batch size, word embedding length)\n",
    "        embedded = self.embedding(word_inputs).view(seq_len, 1, -1)\n",
    "        \n",
    "        # GRU:\n",
    "        #    input: (length of sequence, batch size, size of var in sequence)\n",
    "        #    output: \n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        hidden = Variable(torch.zeros(self.n_layers, 1, self.hidden_size))\n",
    "        if USE_CUDA: hidden = hidden.cuda()\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class classifierNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(classifierNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.i2o = nn.Linear(input_size, output_size)\n",
    "    \n",
    "    def forward(self, result_input):\n",
    "        return self.i2o(result_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's our training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "clip = 5.0\n",
    "\n",
    "def train(input_var, target, rnn, optimizer, criterion):\n",
    "\n",
    "    # Zero gradients of both optimizers\n",
    "    optimizer.zero_grad()\n",
    "    loss = 0 # Added onto for each word\n",
    "\n",
    "    # Get size of input and target sentences\n",
    "    # input_length = input_variable.size()[0]\n",
    "\n",
    "    # Run sentence through rnn\n",
    "    hidden_var = rnn.init_hidden()\n",
    "    output_var, hidden_var = rnn(input_var, hidden_var)\n",
    "    \n",
    "    # compute loss\n",
    "    loss = criterion(output_var[-1], target)\n",
    "\n",
    "    # run backprop\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm(rnn.parameters(), clip)\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions for time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def as_minutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def time_since(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (as_minutes(s), as_minutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we actually train the RNN  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    "after we initialize stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 500 # size of each word embedding\n",
    "n_layers = 2 # number of layers for the RNN\n",
    "dropout_p = 0.05 # we never use this\n",
    "\n",
    "# Initialize the RNN\n",
    "rnn = RNN(n_words, hidden_size, n_layers)\n",
    "\n",
    "num_topics = 24\n",
    "\n",
    "# Initialize the classifier NN\n",
    "classifier = classifierNN(hidden_size, num_topics)\n",
    "\n",
    "# Move models to GPU\n",
    "if USE_CUDA:\n",
    "    rnn.cuda()\n",
    "\n",
    "# Initialize optimizers and criterion\n",
    "learning_rate = 0.0001\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Configuring training\n",
    "n_epochs = 1000\n",
    "plot_every = 20\n",
    "print_every = 10\n",
    "\n",
    "# Keep track of time elapsed and running averages\n",
    "plot_losses = []\n",
    "print_loss_total = 0 # Reset every print_every\n",
    "plot_loss_total = 0 # Reset every plot_every"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aaaaand now we actually start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.0718 -0.0444  0.0503  0.1155  0.2382 -0.1270 -0.2175 -0.0405 -0.0161 -0.1410\n",
      "\n",
      "Columns 10 to 19 \n",
      "-0.0534  0.2328  0.0237  0.0722 -0.0093  0.0970  0.0476  0.1252 -0.0963  0.1511\n",
      "\n",
      "Columns 20 to 29 \n",
      "-0.1360 -0.1065  0.0534  0.1789  0.1194  0.1297 -0.0641 -0.1509  0.0713 -0.0513\n",
      "\n",
      "Columns 30 to 39 \n",
      " 0.0520 -0.0236  0.1997 -0.0050 -0.0200 -0.1702 -0.0723 -0.0382 -0.0044 -0.0123\n",
      "\n",
      "Columns 40 to 49 \n",
      " 0.2125 -0.0207  0.1366  0.0581  0.1211 -0.0630 -0.1311 -0.0710 -0.0072 -0.0984\n",
      "\n",
      "Columns 50 to 59 \n",
      "-0.0053 -0.0205  0.0473  0.0987  0.0676 -0.1871 -0.2164 -0.1523 -0.0425  0.2094\n",
      "\n",
      "Columns 60 to 69 \n",
      " 0.1477 -0.1624  0.0255  0.1580 -0.1315  0.1256  0.1374  0.2881 -0.0272  0.0577\n",
      "\n",
      "Columns 70 to 79 \n",
      "-0.2502  0.2343 -0.0010  0.2240 -0.0500 -0.1467  0.1655 -0.0345  0.0009  0.2359\n",
      "\n",
      "Columns 80 to 89 \n",
      " 0.1226 -0.0169  0.0172 -0.0532 -0.1290  0.0558 -0.0156  0.0082 -0.0543 -0.0960\n",
      "\n",
      "Columns 90 to 99 \n",
      "-0.1444  0.0901 -0.0839 -0.0914  0.0138  0.0758 -0.0102  0.1029  0.1095 -0.0223\n",
      "\n",
      "Columns 100 to 109 \n",
      "-0.0483 -0.0434 -0.0242  0.0592 -0.0249 -0.0515  0.0385  0.1958  0.0021 -0.0236\n",
      "\n",
      "Columns 110 to 119 \n",
      " 0.0580 -0.1642 -0.0166  0.0913  0.0081 -0.1977  0.0840  0.1310 -0.0702 -0.0430\n",
      "\n",
      "Columns 120 to 129 \n",
      " 0.3140 -0.0280  0.0089 -0.1432  0.0680 -0.1125 -0.0321 -0.2290  0.0516  0.1110\n",
      "\n",
      "Columns 130 to 139 \n",
      " 0.2009 -0.0291  0.1904  0.0090 -0.1775 -0.1131  0.0626 -0.1311  0.1352 -0.0893\n",
      "\n",
      "Columns 140 to 149 \n",
      "-0.0282  0.1995  0.0341  0.0627  0.0250 -0.0171 -0.0350  0.0833 -0.0601  0.0253\n",
      "\n",
      "Columns 150 to 159 \n",
      "-0.0425 -0.2198  0.0950  0.0594 -0.0102  0.1305 -0.0872 -0.0984  0.0719  0.1176\n",
      "\n",
      "Columns 160 to 169 \n",
      " 0.0245 -0.1639 -0.0466  0.0272  0.0710 -0.0725  0.0897 -0.0044  0.0843  0.0358\n",
      "\n",
      "Columns 170 to 179 \n",
      "-0.0556  0.1138  0.0758 -0.0206  0.0585 -0.1424 -0.0849  0.0596 -0.1473  0.0295\n",
      "\n",
      "Columns 180 to 189 \n",
      " 0.1813 -0.1525 -0.0126  0.1990  0.1467  0.1016 -0.1039 -0.1205  0.1733 -0.1151\n",
      "\n",
      "Columns 190 to 199 \n",
      "-0.1185  0.0325  0.0762 -0.0848 -0.0677 -0.1593 -0.3311 -0.0984 -0.0181 -0.0848\n",
      "\n",
      "Columns 200 to 209 \n",
      " 0.2217  0.1504  0.0642  0.0701  0.0145  0.2151 -0.1027  0.0457 -0.1944 -0.1107\n",
      "\n",
      "Columns 210 to 219 \n",
      "-0.0109  0.1706  0.1196 -0.1867  0.0806  0.0738  0.0248  0.0402 -0.1535  0.0555\n",
      "\n",
      "Columns 220 to 229 \n",
      " 0.1050  0.0178  0.0618  0.2346 -0.0595  0.0951 -0.0255 -0.0684 -0.0581  0.0938\n",
      "\n",
      "Columns 230 to 239 \n",
      " 0.0854 -0.1710  0.0514 -0.2363 -0.0531  0.0975 -0.1214  0.0076 -0.1707  0.1128\n",
      "\n",
      "Columns 240 to 249 \n",
      " 0.0737  0.0058  0.0262  0.1170 -0.0680  0.0139 -0.0526  0.0178  0.1990 -0.1133\n",
      "\n",
      "Columns 250 to 259 \n",
      "-0.0879 -0.0924 -0.0256  0.0889 -0.0165  0.0193 -0.1339 -0.1928 -0.1309  0.0006\n",
      "\n",
      "Columns 260 to 269 \n",
      " 0.0355  0.1593  0.0484 -0.0232  0.0569  0.0025  0.0553  0.2183  0.1830 -0.0009\n",
      "\n",
      "Columns 270 to 279 \n",
      "-0.1354 -0.1542  0.0986  0.1343  0.0822  0.0361  0.2518  0.1376  0.0637 -0.0949\n",
      "\n",
      "Columns 280 to 289 \n",
      "-0.0244 -0.1016  0.0546  0.0260 -0.1595 -0.1379 -0.0987  0.0427 -0.0207 -0.0570\n",
      "\n",
      "Columns 290 to 299 \n",
      " 0.0284 -0.0177  0.0965 -0.0872 -0.0789  0.0620 -0.0617 -0.0322  0.1087  0.0320\n",
      "\n",
      "Columns 300 to 309 \n",
      " 0.1810 -0.1121 -0.1440  0.1722  0.0863 -0.0661  0.0624  0.2998  0.0376 -0.1207\n",
      "\n",
      "Columns 310 to 319 \n",
      "-0.0772 -0.2869 -0.1802 -0.0420  0.0590 -0.1415  0.0284  0.0251  0.0797  0.0550\n",
      "\n",
      "Columns 320 to 329 \n",
      "-0.0373  0.0138  0.0633 -0.0989  0.0317 -0.0565  0.2508 -0.0135  0.1750 -0.0525\n",
      "\n",
      "Columns 330 to 339 \n",
      "-0.1133  0.0769  0.0626  0.2570 -0.0828  0.2492 -0.0000 -0.1286 -0.0263  0.1191\n",
      "\n",
      "Columns 340 to 349 \n",
      " 0.1679 -0.0016  0.1118  0.0150 -0.1040  0.0004  0.0014  0.0896  0.0596  0.0294\n",
      "\n",
      "Columns 350 to 359 \n",
      " 0.1585  0.0023  0.0082  0.0745  0.0212 -0.1126 -0.0111  0.0223  0.0210  0.2164\n",
      "\n",
      "Columns 360 to 369 \n",
      " 0.0759 -0.0269 -0.0183  0.0649 -0.3169 -0.0686  0.1609 -0.2089  0.1931 -0.0468\n",
      "\n",
      "Columns 370 to 379 \n",
      " 0.0632 -0.0469 -0.0468  0.0634  0.0464  0.0358  0.0743  0.2055  0.0782  0.0748\n",
      "\n",
      "Columns 380 to 389 \n",
      " 0.0229 -0.0023 -0.1023 -0.0470  0.0416 -0.0160 -0.0910 -0.0744  0.2318  0.1084\n",
      "\n",
      "Columns 390 to 399 \n",
      "-0.1275 -0.0631  0.0946  0.0814 -0.0629 -0.0800 -0.0272 -0.1201 -0.2169 -0.1179\n",
      "\n",
      "Columns 400 to 409 \n",
      "-0.0272 -0.1353 -0.1544  0.0708  0.0125 -0.1004  0.0650  0.1090  0.1884  0.1208\n",
      "\n",
      "Columns 410 to 419 \n",
      "-0.1684 -0.2124 -0.1146  0.1835 -0.0251  0.1815 -0.1699 -0.1702 -0.0873 -0.0543\n",
      "\n",
      "Columns 420 to 429 \n",
      "-0.1464 -0.2025 -0.0427 -0.2288  0.0679  0.0382 -0.0385 -0.1041  0.1715 -0.0134\n",
      "\n",
      "Columns 430 to 439 \n",
      " 0.0487  0.1282  0.1409  0.1141 -0.2100  0.1097 -0.2941  0.1348 -0.0128  0.0393\n",
      "\n",
      "Columns 440 to 449 \n",
      "-0.0692  0.1525 -0.3237 -0.0557 -0.1827  0.0129 -0.0993 -0.0158  0.1387 -0.1685\n",
      "\n",
      "Columns 450 to 459 \n",
      " 0.1378  0.0568  0.3202 -0.1168 -0.0231  0.1013  0.0153 -0.0867 -0.0739  0.0039\n",
      "\n",
      "Columns 460 to 469 \n",
      "-0.0733 -0.0157 -0.1996 -0.1568  0.0447 -0.0308 -0.0832 -0.0511  0.2310 -0.1062\n",
      "\n",
      "Columns 470 to 479 \n",
      "-0.1380 -0.1288  0.0881 -0.0552  0.0500  0.1863 -0.1350  0.2829 -0.0370  0.0782\n",
      "\n",
      "Columns 480 to 489 \n",
      "-0.0847 -0.0013  0.2291 -0.2688 -0.0211  0.0605 -0.0962  0.0286  0.0469 -0.0794\n",
      "\n",
      "Columns 490 to 499 \n",
      " 0.1280  0.0893  0.1123  0.2552  0.1556  0.1460  0.1453 -0.0939  0.2683 -0.0544\n",
      "[torch.FloatTensor of size 1x500]\n",
      "\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "nn criterions don't compute the gradient w.r.t. targets - please mark these variables as volatile or not requiring gradients",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-230eab10904a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Run the train function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_distribution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Keep track of loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-aec49ca3b7fb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_var, target, rnn, optimizer, criterion)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_var\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# run backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0m_assert_no_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m         return F.nll_loss(input, target, self.weight, self.size_average,\n\u001b[1;32m    132\u001b[0m                           self.ignore_index)\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36m_assert_no_grad\u001b[0;34m(variable)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_assert_no_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;34m\"nn criterions don't compute the gradient w.r.t. targets - please \"\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;34m\"mark these variables as volatile or not requiring gradients\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: nn criterions don't compute the gradient w.r.t. targets - please mark these variables as volatile or not requiring gradients"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    \n",
    "    # Get training data for this cycle\n",
    "    sentence, distribution = choose_random_training_pair()\n",
    "    \n",
    "    training_sentence = variable_from_sentence(vocabulary, sentence)\n",
    "    target_distribution = tensor_from_distribution(distribution)\n",
    "\n",
    "    # Run the train function\n",
    "    loss = train(training_sentence, target_distribution, rnn, optimizer, criterion)\n",
    "\n",
    "    # Keep track of loss\n",
    "    print_loss_total += loss\n",
    "    plot_loss_total += loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print_loss_avg = print_loss_total / print_every\n",
    "        print_loss_total = 0\n",
    "        print_summary = '%s (%d %d%%) %.4f' % (time_since(start, epoch / n_epochs), epoch, epoch / n_epochs * 100, print_loss_avg)\n",
    "        print(print_summary)\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        plot_loss_avg = plot_loss_total / plot_every\n",
    "        plot_losses.append(plot_loss_avg)\n",
    "        plot_loss_total = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "def show_plot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    loc = ticker.MultipleLocator(base=0.2) # put ticks at regular intervals\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "\n",
    "show_plot(plot_losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
