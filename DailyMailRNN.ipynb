{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "\n",
    "Given a sequence, predict the topic distribution for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# the dists are saved as comma separated values in a string (because pandas can't save datatype as int)\n",
    "# so to read as numeric-values, you must split on spaces and convert to float.\n",
    "# Michael says there is an efficient way to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>dists</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>By</td>\n",
       "      <td>0.000215766137942 0.000241668597233 0.00020097...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Beth Stebner Tara Brady</td>\n",
       "      <td>0.000117660461174 0.000123021350745 0.00012870...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PUBLISHED :</td>\n",
       "      <td>0.000574640958915 0.000137269205565 0.00015039...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13:57 EST , 21 January 2013</td>\n",
       "      <td>0.000117660461174 0.000123021350745 0.00012870...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>|</td>\n",
       "      <td>0.000215766137942 0.000241668597233 0.00020097...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     sentences  \\\n",
       "0                           By   \n",
       "1      Beth Stebner Tara Brady   \n",
       "2                  PUBLISHED :   \n",
       "3  13:57 EST , 21 January 2013   \n",
       "4                            |   \n",
       "\n",
       "                                               dists  \n",
       "0  0.000215766137942 0.000241668597233 0.00020097...  \n",
       "1  0.000117660461174 0.000123021350745 0.00012870...  \n",
       "2  0.000574640958915 0.000137269205565 0.00015039...  \n",
       "3  0.000117660461174 0.000123021350745 0.00012870...  \n",
       "4  0.000215766137942 0.000241668597233 0.00020097...  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert back to original distribution\n",
    "df['dists'] = df['dists'].str.split(\" \")\n",
    "df['dists'] = df['dists'].apply(lambda x: [float(i) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4406"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUTPUT_SIZE = len(df['dists'][0])\n",
    "OUTPUT_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let the ML begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"6,7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "USE_CUDA = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our vocabulary class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Voc:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2 # Count SOS and EOS\n",
    "      \n",
    "    def index_words(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.index_word(word)\n",
    "\n",
    "    def index_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods for cleaning up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalize_string(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets actually use the data now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SENTENCES = 200\n",
    "\n",
    "# subset of the data for training\n",
    "training_df = df[:MAX_SENTENCES]\n",
    "\n",
    "# choose a random sentence and its corresponding distribution (label)\n",
    "def choose_random_training_pair():\n",
    "    training_pair = training_df.loc[random.randint(0, training_df.shape[0]-1)]\n",
    "\n",
    "    sentence = training_pair['sentences']\n",
    "    distribution = training_pair['dists']\n",
    "\n",
    "    return sentence, distribution\n",
    "\n",
    "def generate_vocabulary(sentences):\n",
    "    # create the vocabulary\n",
    "    vocabulary = Voc('source_identification')\n",
    "    for sentence in sentences:\n",
    "        vocabulary.index_words(sentence)\n",
    "\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary = generate_vocabulary(training_df['sentences'].tolist())\n",
    "n_words = vocabulary.n_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets actually do some ML now.\n",
    "\n",
    "Here are some sentences --> variable functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Return a list of indexes, one for each word in the sentence\n",
    "def indexes_from_sentence(voc, sentence):\n",
    "    return [voc.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def variable_from_sentence(voc, sentence):\n",
    "    indexes = indexes_from_sentence(voc, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    var = Variable(torch.LongTensor(indexes).view(-1, 1))\n",
    "    if USE_CUDA: var = var.cuda()\n",
    "    return var\n",
    "\n",
    "def tensor_from_distribution(distribution):\n",
    "    return Variable(torch.FloatTensor(distribution).view(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now here's the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # embedding contains input_size vectors of size hidden_size.\n",
    "        # this performs the word embeddings\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "        \n",
    "    def forward(self, word_inputs, hidden):\n",
    "        # Note: we run this all at once (over the whole input sequence)\n",
    "        seq_len = len(word_inputs)\n",
    "        \n",
    "        # embedding: (sentence length, batch size, word embedding length)\n",
    "        embedded = self.embedding(word_inputs).view(seq_len, 1, -1)\n",
    "        \n",
    "        # GRU:\n",
    "        #    input: (length of sequence, batch size, size of var in sequence)\n",
    "        #    output: \n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        hidden = Variable(torch.zeros(self.n_layers, 1, self.hidden_size))\n",
    "        if USE_CUDA: hidden = hidden.cuda()\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class classifierNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(classifierNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.i2o = nn.Linear(input_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "    \n",
    "    def forward(self, result_input):\n",
    "        output = self.i2o(result_input)\n",
    "        output = self.softmax(output).view(-1, 1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's our training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "clip = 5.0\n",
    "\n",
    "def train(input_var, target, rnn, rnn_optimizer, classifier_optimizer, criterion):\n",
    "\n",
    "    # Zero gradients of both optimizers\n",
    "    optimizer.zero_grad()\n",
    "    loss = 0 # Added onto for each word\n",
    "\n",
    "    # Get size of input and target sentences\n",
    "    # input_length = input_variable.size()[0]\n",
    "\n",
    "    # Run sentence through rnn\n",
    "    hidden_var = rnn.init_hidden()\n",
    "    output_var, hidden_var = rnn(input_var, hidden_var)\n",
    "    \n",
    "    # run classification rnn to get distribution\n",
    "    output_distribution = classifier(output_var[-1])\n",
    "    \n",
    "    # compute loss\n",
    "    loss = criterion(output_distribution, target)\n",
    "\n",
    "    # run backprop\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm(rnn.parameters(), clip)\n",
    "    \n",
    "    rnn_optimizer.step()\n",
    "    classifier_optimizer.step()\n",
    "    \n",
    "    return loss.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions for time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def as_minutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def time_since(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (as_minutes(s), as_minutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we actually train the RNN  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    "after we initialize stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 500 # size of each word embedding\n",
    "n_layers = 2 # number of layers for the RNN\n",
    "dropout_p = 0.05 # we never use this\n",
    "\n",
    "# Initialize the RNN\n",
    "rnn = RNN(n_words, hidden_size, n_layers)\n",
    "\n",
    "num_topics = 24\n",
    "\n",
    "# Initialize the classifier NN\n",
    "classifier = classifierNN(hidden_size, OUTPUT_SIZE)\n",
    "\n",
    "# Move models to GPU\n",
    "if USE_CUDA:\n",
    "    rnn.cuda()\n",
    "\n",
    "# Initialize optimizers and criterion\n",
    "learning_rate = 0.0001\n",
    "rnn_optimizer = optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "classifier_optimizer = optim.Adam(classifier.parameters(), lr=learning_rate)\n",
    "criterion = nn.KLDivLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Configuring training\n",
    "n_epochs = 1000\n",
    "plot_every = 20\n",
    "print_every = 10\n",
    "\n",
    "# Keep track of time elapsed and running averages\n",
    "plot_losses = []\n",
    "print_loss_total = 0 # Reset every print_every\n",
    "plot_loss_total = 0 # Reset every plot_every"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aaaaand now we actually start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 3s (- 5m 9s) (10 1%) 0.0000755474\n",
      "0m 5s (- 4m 49s) (20 2%) 0.0000798607\n",
      "0m 8s (- 4m 42s) (30 3%) 0.0000636696\n",
      "0m 12s (- 4m 56s) (40 4%) 0.0000633480\n",
      "0m 16s (- 5m 5s) (50 5%) 0.0000676324\n",
      "0m 18s (- 4m 55s) (60 6%) 0.0000457608\n",
      "0m 22s (- 4m 55s) (70 7%) 0.0000536902\n",
      "0m 25s (- 4m 47s) (80 8%) 0.0000583254\n",
      "0m 28s (- 4m 48s) (90 9%) 0.0000520081\n",
      "0m 31s (- 4m 47s) (100 10%) 0.0000705582\n",
      "0m 35s (- 4m 44s) (110 11%) 0.0000678794\n",
      "0m 39s (- 4m 47s) (120 12%) 0.0000659909\n",
      "0m 41s (- 4m 38s) (130 13%) 0.0000591950\n",
      "0m 43s (- 4m 28s) (140 14%) 0.0000503005\n",
      "0m 45s (- 4m 20s) (150 15%) 0.0000415671\n",
      "0m 48s (- 4m 15s) (160 16%) 0.0000662836\n",
      "0m 51s (- 4m 9s) (170 17%) 0.0001037342\n",
      "0m 53s (- 4m 5s) (180 18%) 0.0000800503\n",
      "0m 56s (- 3m 59s) (190 19%) 0.0000534334\n",
      "0m 59s (- 3m 59s) (200 20%) 0.0000549219\n",
      "1m 1s (- 3m 52s) (210 21%) 0.0000554445\n",
      "1m 5s (- 3m 51s) (220 22%) 0.0000827069\n",
      "1m 7s (- 3m 45s) (230 23%) 0.0000428872\n",
      "1m 9s (- 3m 39s) (240 24%) 0.0000889584\n",
      "1m 12s (- 3m 37s) (250 25%) 0.0000956379\n",
      "1m 14s (- 3m 32s) (260 26%) 0.0000838078\n",
      "1m 18s (- 3m 31s) (270 27%) 0.0000530057\n",
      "1m 21s (- 3m 29s) (280 28%) 0.0000836716\n",
      "1m 24s (- 3m 26s) (290 28%) 0.0000767516\n",
      "1m 26s (- 3m 21s) (300 30%) 0.0000706850\n",
      "1m 28s (- 3m 17s) (310 31%) 0.0000505693\n",
      "1m 31s (- 3m 14s) (320 32%) 0.0000979538\n",
      "1m 33s (- 3m 10s) (330 33%) 0.0000712353\n",
      "1m 36s (- 3m 6s) (340 34%) 0.0000451513\n",
      "1m 38s (- 3m 2s) (350 35%) 0.0000280136\n",
      "1m 41s (- 3m 0s) (360 36%) 0.0000459877\n",
      "1m 43s (- 2m 56s) (370 37%) 0.0000688410\n",
      "1m 46s (- 2m 53s) (380 38%) 0.0001030508\n",
      "1m 48s (- 2m 50s) (390 39%) 0.0000767178\n",
      "1m 51s (- 2m 47s) (400 40%) 0.0000722449\n",
      "1m 54s (- 2m 45s) (410 41%) 0.0000696975\n",
      "1m 57s (- 2m 42s) (420 42%) 0.0000765872\n",
      "2m 0s (- 2m 40s) (430 43%) 0.0000669060\n",
      "2m 3s (- 2m 37s) (440 44%) 0.0000556287\n",
      "2m 6s (- 2m 34s) (450 45%) 0.0000747592\n",
      "2m 9s (- 2m 32s) (460 46%) 0.0000457571\n",
      "2m 11s (- 2m 28s) (470 47%) 0.0000780822\n",
      "2m 13s (- 2m 25s) (480 48%) 0.0000981161\n",
      "2m 16s (- 2m 22s) (490 49%) 0.0001017218\n",
      "2m 20s (- 2m 20s) (500 50%) 0.0000891136\n",
      "2m 22s (- 2m 16s) (510 51%) 0.0000811198\n",
      "2m 26s (- 2m 14s) (520 52%) 0.0000706119\n",
      "2m 28s (- 2m 11s) (530 53%) 0.0000594105\n",
      "2m 31s (- 2m 9s) (540 54%) 0.0000678324\n",
      "2m 35s (- 2m 7s) (550 55%) 0.0000712540\n",
      "2m 38s (- 2m 4s) (560 56%) 0.0000698639\n",
      "2m 40s (- 2m 1s) (570 56%) 0.0000706656\n",
      "2m 43s (- 1m 58s) (580 57%) 0.0000557259\n",
      "2m 46s (- 1m 55s) (590 59%) 0.0001236844\n",
      "2m 49s (- 1m 52s) (600 60%) 0.0000760081\n",
      "2m 53s (- 1m 50s) (610 61%) 0.0000703157\n",
      "2m 56s (- 1m 48s) (620 62%) 0.0000559041\n",
      "3m 0s (- 1m 45s) (630 63%) 0.0000605025\n",
      "3m 3s (- 1m 42s) (640 64%) 0.0000603840\n",
      "3m 7s (- 1m 40s) (650 65%) 0.0001015284\n",
      "3m 12s (- 1m 38s) (660 66%) 0.0000513903\n",
      "3m 15s (- 1m 36s) (670 67%) 0.0000682922\n",
      "3m 19s (- 1m 34s) (680 68%) 0.0000423381\n",
      "3m 23s (- 1m 31s) (690 69%) 0.0000580543\n",
      "3m 26s (- 1m 28s) (700 70%) 0.0000630195\n",
      "3m 30s (- 1m 25s) (710 71%) 0.0000566462\n",
      "3m 34s (- 1m 23s) (720 72%) 0.0001090628\n",
      "3m 37s (- 1m 20s) (730 73%) 0.0000602288\n",
      "3m 40s (- 1m 17s) (740 74%) 0.0000535588\n",
      "3m 44s (- 1m 14s) (750 75%) 0.0000389638\n",
      "3m 47s (- 1m 11s) (760 76%) 0.0000280960\n",
      "3m 50s (- 1m 8s) (770 77%) 0.0000250991\n",
      "3m 53s (- 1m 5s) (780 78%) 0.0000681439\n",
      "3m 57s (- 1m 3s) (790 79%) 0.0000938238\n",
      "4m 0s (- 1m 0s) (800 80%) 0.0000227630\n",
      "4m 3s (- 0m 57s) (810 81%) 0.0000981924\n",
      "4m 6s (- 0m 54s) (820 82%) 0.0000712710\n",
      "4m 10s (- 0m 51s) (830 83%) 0.0000171071\n",
      "4m 13s (- 0m 48s) (840 84%) 0.0000285660\n",
      "4m 16s (- 0m 45s) (850 85%) 0.0000541683\n",
      "4m 19s (- 0m 42s) (860 86%) 0.0000886809\n",
      "4m 22s (- 0m 39s) (870 87%) 0.0000323614\n",
      "4m 24s (- 0m 36s) (880 88%) 0.0001231535\n",
      "4m 28s (- 0m 33s) (890 89%) 0.0000443168\n",
      "4m 32s (- 0m 30s) (900 90%) 0.0000678680\n",
      "4m 36s (- 0m 27s) (910 91%) 0.0000341461\n",
      "4m 39s (- 0m 24s) (920 92%) 0.0001250362\n",
      "4m 42s (- 0m 21s) (930 93%) 0.0000396996\n",
      "4m 44s (- 0m 18s) (940 94%) 0.0000322257\n",
      "4m 47s (- 0m 15s) (950 95%) 0.0000332355\n",
      "4m 50s (- 0m 12s) (960 96%) 0.0000433108\n",
      "4m 52s (- 0m 9s) (970 97%) 0.0000507097\n",
      "4m 55s (- 0m 6s) (980 98%) 0.0000818109\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    \n",
    "    # Get training data for this cycle\n",
    "    sentence, distribution = choose_random_training_pair()\n",
    "    \n",
    "    training_sentence = variable_from_sentence(vocabulary, sentence)\n",
    "    target_distribution = tensor_from_distribution(distribution)\n",
    "\n",
    "    # Run the train function\n",
    "    loss = train(training_sentence, target_distribution, rnn, rnn_optimizer, classifier_optimizer, criterion)\n",
    "\n",
    "    # Keep track of loss\n",
    "    print_loss_total += loss\n",
    "    plot_loss_total += loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print_loss_avg = print_loss_total / print_every\n",
    "        print_loss_total = 0\n",
    "        print_summary = '%s (%d %d%%) %.10f' % (time_since(start, epoch / n_epochs), epoch, epoch / n_epochs * 100, print_loss_avg)\n",
    "        print(print_summary)\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        plot_loss_avg = plot_loss_total / plot_every\n",
    "        plot_losses.append(plot_loss_avg)\n",
    "        plot_loss_total = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "def show_plot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    loc = ticker.MultipleLocator(base=0.2) # put ticks at regular intervals\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "\n",
    "show_plot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
